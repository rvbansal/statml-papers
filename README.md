Some foundational papers to read in different areas of ML and statistics.

## Overparameterized Models + Interpolation Regime
* Bartlett 2020 - [Benign Overfitting in Linear Regression](https://arxiv.org/abs/1906.11300)
* Bartlett, Montanari & Rakhlin 2021 - [Deep learning: a statistical viewpoint](https://arxiv.org/abs/2103.09177)
* Belkin, Hsu & Xu 2019 - [Two models of double descent for weak features](https://arxiv.org/abs/1903.07571)
* Nakkiran et al. 2021 - [Optimal Regularization Can Mitigate Double Descent](https://arxiv.org/abs/2003.01897)
* Nakkiran 2019 - [More Data Can Hurt for Linear Regression: Sample-wise Double Descent](https://arxiv.org/abs/1912.07242)
* Belkin 2021 - [Fit without fear: remarkable mathematical phenomena of deep learning through the prism of interpolation](https://arxiv.org/abs/2105.14368)
* Belkin et al. 2019 - [Reconciling modern machine learning and the bias-variance trade-off](https://arxiv.org/abs/1812.11118)
* Hastie et al. 2020 - [Surprises in High-Dimensional Ridgeless Least Squares Interpolation](https://arxiv.org/abs/1903.08560)
* Mei & Montanari 2021 - [The generalization error of random features regression:
Precise asymptotics and double descent curve](https://arxiv.org/pdf/1908.05355.pdf)
* Xu & Hsu 2019 - [On the number of variables to use in principal component regression](https://arxiv.org/abs/1906.01139)
* Muthukumar et al. 2019 - [Harmless interpolation of noisy data in regression](https://arxiv.org/abs/1903.09139)
* Dar, Muthukumar & Baraniuk 2021 - [A Farewell to the Bias-Variance Tradeoff? An Overview of the Theory of Overparameterized Machine Learning](https://arxiv.org/pdf/2109.02355.pdf)
* Wyner et al. 2017 - [Explaining the Success of AdaBoost and Random Forests as Interpolating Classifiers](https://jmlr.org/papers/v18/15-240.html)

## Deep Learning Generalization
* Belkin, Ma & Mandal 2018 - [To Understand Deep Learning We Need to Understand Kernel Learning](https://arxiv.org/pdf/1802.01396.pdf)
* Zhang et al. 2017 - [Understanding deep learning requires rethinking generalization](https://arxiv.org/abs/1611.03530)
* Neyshabur et al. 2018 - [Towards Understanding the Role of Over-Parametrization in Generalization of Neural Networks](https://arxiv.org/abs/1805.12076)
* Nakkiran et al. 2020 - [The Deep Bootstrap Framework: Good Online Learners are Good Offline Generalizers](https://arxiv.org/abs/2010.08127)
* Yang et al. 2020 - [Rethinking Bias-Variance Trade-off for Generalization of Neural Networks](https://arxiv.org/pdf/2002.11328.pdf)

## Robustness
* Lugosi & Mendelson 2019 - [Mean estimation and regression under heavy-tailed distributions--a survey](https://arxiv.org/abs/1906.04280)
* Prasad et al. 2018 - [Robust Estimation via Robust Gradient Estimation](https://arxiv.org/abs/1802.06485)
* Diakonikolas et al. 2019 - [Robust Estimators in High Dimensions without the Computational Intractability](https://arxiv.org/abs/1604.06443)
* Lai, Rao & Vempala 2016 - [Agnostic Estimation of Mean and Covariance](https://arxiv.org/abs/1604.06968)
* Diakonikolas et al. 2019 - [Sever: A Robust Meta-Algorithm for Stochastic Optimization](https://arxiv.org/abs/1803.02815)
* Gao et al. 2018 - [Robust Estimation and Generative Adversarial Nets](https://arxiv.org/abs/1810.02030)
* Minsker 2013 - [Geometric median and robust estimation in Banach spaces](https://arxiv.org/abs/1308.1334)
* Donoho & Liu 1988 - [The "Automatic" Robustness of Minimum Distance Functionals](https://projecteuclid.org/journals/annals-of-statistics/volume-16/issue-2/The-Automatic-Robustness-of-Minimum-Distance-Functionals/10.1214/aos/1176350820.full)
* Donoho & Liu 1991 - [Geometrizing Rates of Convergence](http://www.stat.yale.edu/~pollard/Courses/610.fall2014/Handouts/DonohoLiu87Geom1.pdf)
* Hopkins, Li & Zhang 2019 - [Robust and Heavy-Tailed Mean Estimation Made Simple, via Regret Minimization](https://arxiv.org/abs/2007.15839)
* Lecue et al. 2020 - [Robust classification via MOM minimization](https://link.springer.com/content/pdf/10.1007/s10994-019-05863-6.pdf)

## Conformal Prediction
* Bates et al. 2021 - [Distribution-Free, Risk-Controlling Prediction Sets](https://arxiv.org/abs/2101.02703)
* Lei et al. 2018 - [Distribution-Free Predictive Inference for Regression](https://arxiv.org/abs/1604.04173)
* Gupta, Podkopaev & Ramdas 2020 - [Distribution-free binary classification: prediction sets, confidence intervals and calibration](https://arxiv.org/abs/2006.10564)
* Cauchois et al. 2020 - [Robust Validation: Confident Predictions Even When Distributions Shift](https://arxiv.org/abs/2008.04267)
* Barber et al. 2020 - [The limits of distribution-free conditional predictive inference](https://arxiv.org/abs/1903.04684)
* Barber et al. 2019 - [Predictive inference with the jackknife+](https://arxiv.org/abs/1905.02928)
* Tibshirani et al. 2019 - [Conformal Prediction Under Covariate Shift](https://arxiv.org/abs/1904.06019)
* Angelopoulos et al. 2020 - [Uncertainty Sets for Image Classifiers using Conformal Prediction](https://arxiv.org/abs/2009.14193)

## Distribution Shift
* Ben-David et al. 2009 - [A theory of learning from different domains](http://www.alexkulesza.com/pubs/adapt_mlj10.pdf)
* Ganin et al. 2016 - [Domain-Adversarial Training of Neural Networks](https://arxiv.org/abs/1505.07818)
* Zhao et al. 2019 - [On Learning Invariant Representation for Domain Adaptation](https://arxiv.org/pdf/1901.09453.pdf)
* Shimodaira 2000 - [Improving predictive inference under covariate shift by weighting the log-likelihood function](https://www.sciencedirect.com/science/article/abs/pii/S0378375800001154)
* Saerens, Latinne & Decaesteker 2002 - [Adjusting the outputs of a classifier to new a priori probabilities: a simple procedure](https://pubmed.ncbi.nlm.nih.gov/11747533/)
* Courty et al. 2015 - [Optimal Transport for Domain Adaptation](https://arxiv.org/abs/1507.00504)
* Johansson, Sontag & Ranganath 2019 - [Support and Invertibility in Domain-Invariant Representations](https://arxiv.org/abs/1903.03448)
* Henzie-Deml & Meinhausen 2017 - [Conditional Variance Penalties and Domain Shift Robustness](https://arxiv.org/abs/1710.11469)
* Arjovsky et al. 2020 - [Invariant Risk Minimization](https://arxiv.org/abs/1907.02893)
* Rosenfeld, Ravikumar & Risteski 2021 - [The Risks of Invariant Risk Minimization](https://arxiv.org/abs/2010.05761)
* Peters, Buhlmann & Meinhausen 2015 - [Causal inference using invariant prediction: identification and confidence intervals](https://arxiv.org/abs/1501.01332)
* Azizzadenesheli et al. 2019 - [Regularized Learning for Domain Adaptation under Label Shifts](https://arxiv.org/abs/1903.09734)

## Deep Learning Optimization
* Liu, Zhu & Belkin 2020 - [Loss landscapes and optimization in over-parameterized non-linear systems and neural networks](https://arxiv.org/abs/2003.00307)
* Dauphin et al. 2017 - [Identifying and attacking the saddle point problem in high-dimensional non-convex optimization](https://arxiv.org/abs/1406.2572)
* Du et al. 2019 - [Gradient Descent Finds Global Minima of Deep Neural Networks](https://arxiv.org/abs/1811.03804)
* Soltanolkotabi, Javanmard & Lee 2018 - [Theoretical insights into the optimization landscape of over-parameterized shallow neural networks](https://arxiv.org/abs/1707.04926)
* Arora, Cohen & Hazan 2018 - [On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization](https://arxiv.org/abs/1802.06509)
* Kleinberg, Li & Yuan 2018 - [An Alternative View: When Does SGD Escape Local Minima?](https://arxiv.org/abs/1802.06175)
* Allen-Zhu, Li & Song 2018 - [A Convergence Theory for Deep Learning via Over-Parameterization](https://arxiv.org/abs/1811.03962)

## NTK Regime
* Jacot, Gabriel & Hongler 2018 - [Neural Tangent Kernel: Convergence and Generalization in Neural Networks](https://arxiv.org/abs/1806.07572)
* Arora et al. 2019 - [Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks](https://arxiv.org/abs/1910.01663)
* Arora et al. 2019 - [On Exact Computation with an Infinitely Wide Neural Net](https://arxiv.org/abs/1904.11955)
* Lee et al. 2017 - [Deep Neural Networks as Gaussian Processes](https://arxiv.org/abs/1711.00165)
* Matthews et al. 2018 - [Gaussian Process Behaviour in Wide Deep Neural Networks](https://arxiv.org/abs/1804.11271)
* Chizat, Oyallon & Bach 2018 - [On Lazy Training in Differentiable Programming](https://arxiv.org/abs/1812.07956)
* Woodworth et al. 2019 - [Kernel and Rich Regimes in Overparametrized Models](https://arxiv.org/abs/1906.05827)

## Algorithmic Regularization
* Soudry et al. 2017 - [The Implicit Bias of Gradient Descent on Separable Data](https://arxiv.org/abs/1710.10345)
* Ji & Telgarsky 2018 - [Risk and parameter convergence of logistic regression](https://arxiv.org/abs/1803.07300)
* Telgarsky 2013 - [Margins, Shrinkage, and Boosting](https://arxiv.org/abs/1303.4172)
* Gunasekar et al. 2018 - [Characterizing Implicit Bias in Terms of Optimization Geometry](https://arxiv.org/abs/1802.08246)
* Ali, Kolter & Tibshirani 2018 - [A Continuous-Time View of Early Stopping for Least Squares](https://arxiv.org/abs/1810.10082)
* Ali, Kolter & Tibshirani 2020 - [The Implicit Regularization of Stochastic Gradient Flow for Least Squares](https://arxiv.org/abs/2003.07802)
* Srebro, Rennie & Jaakkola 2004 - [Maximum-Margin Matrix Factorization](https://papers.nips.cc/paper/2004/file/e0688d13958a19e087e123148555e4b4-Paper.pdf)

## Applied Deep Learning
* D'Armour et al. 2020 - [Underspecification Presents Challenges for Credibility in Modern Machine Learning](https://arxiv.org/abs/2011.03395)
* Santurkar et al. 2019 - [How Does Batch Normalization Help Optimization?](https://arxiv.org/pdf/1805.11604.pdf)
* Srivastava et al. 2014 - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)
* Li et al. 2018 - [Understanding the Disharmony between Dropout and Batch Normalization by Variance Shift](https://arxiv.org/abs/1801.05134)
* Bergsta & Bengio 2012 - [Random Search for Hyper-Parameter Optimization](https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a)
* Hara, Saitoh & Shouno 2017 - [Analysis of dropout learning regarded as ensemble learning](https://arxiv.org/abs/1706.06859)
* Frankle & Carbin 2019 - [The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/abs/1803.03635)
* Frankle et al. 2020 - [Stabilizing the Lottery Ticket Hypothesis](https://arxiv.org/abs/1903.01611)

## Random
* Hooker & Mentsch 2019 - [Please Stop Permuting Features: An Explanation and Alternatives](https://arxiv.org/abs/1905.03151)
* Mentsch & Zhou 2020 - [Randomization as Regularization: A Degrees of Freedom
Explanation for Random Forest Success](https://arxiv.org/pdf/1911.00190.pdf)
* Mentsch & Zhou 2021 - [Trees, Forests, Chickens, and Eggs: When and
Why to Prune Trees in a Random Forest](https://arxiv.org/pdf/2103.16700.pdf)

## Notes
* [Duchi - Information Theory](http://web.stanford.edu/class/stats311/lecture-notes.pdf)
* [Li - Robustness](https://jerryzli.github.io/robust-ml-fall19.html)
* [Steinhardt - Robustness](https://jsteinhardt.stat.berkeley.edu/teaching/stat240-spring-2021)
